# yanbao AI - æ–° Manus è´¦å·æ‰§è¡Œæ‰‹å†Œ

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’Ÿï¼‰

```bash
# 1. å…‹éš†é¡¹ç›®
gh repo clone Tsaojason-cao/yanbao-imaging-studio
cd yanbao-imaging-studio

# 2. å®‰è£…ä¾èµ–
pnpm install

# 3. æŸ¥çœ‹æ‰€æœ‰æ–‡æ¡£
ls -la *.md

# 4. å¼€å§‹æ‰§è¡Œ
# æŒ‰ç…§ä¸‹é¢çš„ Day 1-7 é€æ­¥æ‰§è¡Œ
```

---

## ğŸ“… Day 1: ç¯å¢ƒæ­å»ºä¸æ¶æ„æ ¡å¯¹

### ğŸ¯ ç›®æ ‡
å‰”é™¤è€ä»£ç ä¸­è‡ƒè‚¿ã€è¿Ÿé’çš„ä¼ ç»Ÿé€»è¾‘ï¼Œä¸ºæ™ºèƒ½åŒ–å‡çº§åšå‡†å¤‡

### â° æ—¶é—´åˆ†é…
- ä¸Šåˆ (4h): ä»£ç å®¡è®¡
- ä¸‹åˆ (4h): æ¶æ„é‡æ„

---

### ä¸Šåˆ: ä»£ç å®¡è®¡ (9:00-13:00)

#### Step 1: å…‹éš†å¹¶æ£€æŸ¥é¡¹ç›® (30åˆ†é’Ÿ)

```bash
# 1.1 å…‹éš†é¡¹ç›®
gh repo clone Tsaojason-cao/yanbao-imaging-studio
cd yanbao-imaging-studio

# 1.2 æŸ¥çœ‹é¡¹ç›®ç»“æ„
tree -L 2 -I 'node_modules|.git'

# 1.3 æŸ¥çœ‹æ‰€æœ‰æ–‡æ¡£
ls -la *.md

# 1.4 é˜…è¯»æ ¸å¿ƒæ–‡æ¡£ï¼ˆå¿…è¯»ï¼‰
cat INTELLIGENCE_UPGRADE.md    # æ™ºèƒ½åŒ–å‡çº§æ–¹æ¡ˆ
cat ARCHITECTURE.md             # äº‘ç«¯æ¶æ„
cat MASTER_AND_MEMORY.md        # å¤§å¸ˆåŠŸèƒ½ä¸è®°å¿†ç³»ç»Ÿ
```

#### Step 2: å®‰è£…ä¾èµ– (15åˆ†é’Ÿ)

```bash
# 2.1 å®‰è£… Node.js ä¾èµ–
pnpm install

# 2.2 å®‰è£… Python ä¾èµ–ï¼ˆåç«¯ï¼‰
sudo pip3 install openai pinecone-client redis neo4j

# 2.3 éªŒè¯å®‰è£…
pnpm list
pip3 list | grep -E "openai|pinecone|redis|neo4j"
```

#### Step 3: ä»£ç è´¨é‡åˆ†æ (1å°æ—¶)

```bash
# 3.1 è¿è¡Œ Linter
npm run lint

# 3.2 æŸ¥æ‰¾å­¤ç«‹åŠŸèƒ½
grep -r "export.*function" src/ | grep -v "Memory\|Master\|AI"

# 3.3 æŸ¥æ‰¾ç¡¬ç¼–ç æ•°æ®
grep -r "const.*=.*\[" src/ | grep -v "import"

# 3.4 æŸ¥æ‰¾ç¼ºå°‘è®°å¿†é›†æˆçš„éƒ¨åˆ†
grep -r "camera\|photo\|edit" src/ | grep -v "Memory"
```

#### Step 4: è¯†åˆ«é—®é¢˜ä»£ç  (1.5å°æ—¶)

åˆ›å»ºé—®é¢˜æ¸…å•ï¼š

```bash
# åˆ›å»ºé—®é¢˜æ¸…å•æ–‡ä»¶
cat > /tmp/code_issues.md << 'EOF'
# ä»£ç é—®é¢˜æ¸…å•

## å­¤ç«‹åŠŸèƒ½ï¼ˆéœ€è¦é›†æˆè®°å¿†ï¼‰
- [ ] CameraScreen.tsx - æ‹ç…§åŠŸèƒ½æ²¡æœ‰è®°å¿†é›†æˆ
- [ ] EditorScreen.tsx - ç¼–è¾‘å™¨æ²¡æœ‰é…æ–¹è®°å¿†
- [ ] GalleryScreen.tsx - ç›¸å†Œæ²¡æœ‰æ™ºèƒ½åˆ†ç±»
- [ ] MapScreen.tsx - åœ°å›¾æ²¡æœ‰è¶³è¿¹è®°å¿†

## å†—ä½™ä»£ç ï¼ˆéœ€è¦ä¼˜åŒ–ï¼‰
- [ ] é‡å¤çš„æ•°æ®åº“æŸ¥è¯¢
- [ ] é‡å¤çš„çŠ¶æ€ç®¡ç†
- [ ] é‡å¤çš„æ ·å¼å®šä¹‰

## ç¼ºå¤±åŠŸèƒ½ï¼ˆéœ€è¦æ·»åŠ ï¼‰
- [ ] è®°å¿†æœåŠ¡æ¥å£
- [ ] å¤§å¸ˆæ¨ç†æ¥å£
- [ ] å‘é‡æ£€ç´¢æ¥å£
- [ ] è‡ªè¿›åŒ–åé¦ˆæ”¶é›†
EOF

cat /tmp/code_issues.md
```

#### Step 5: åˆ›å»ºé‡æ„è®¡åˆ’ (1å°æ—¶)

```bash
# åˆ›å»ºé‡æ„è®¡åˆ’
cat > /tmp/refactor_plan.md << 'EOF'
# é‡æ„è®¡åˆ’

## é˜¶æ®µ1: åˆ›å»ºæ ¸å¿ƒæœåŠ¡å±‚
1. åˆ›å»º MemoryService.ts - è®°å¿†æœåŠ¡
2. åˆ›å»º MasterService.ts - å¤§å¸ˆæœåŠ¡
3. åˆ›å»º VectorService.ts - å‘é‡æ£€ç´¢æœåŠ¡

## é˜¶æ®µ2: é‡æ„ç°æœ‰ç»„ä»¶
1. é‡æ„ CameraScreen.tsx - é›†æˆè®°å¿†
2. é‡æ„ EditorScreen.tsx - é›†æˆé…æ–¹è®°å¿†
3. é‡æ„ GalleryScreen.tsx - é›†æˆæ™ºèƒ½åˆ†ç±»
4. é‡æ„ MapScreen.tsx - é›†æˆè¶³è¿¹è®°å¿†

## é˜¶æ®µ3: æ·»åŠ æ™ºèƒ½åŒ–åŠŸèƒ½
1. æ·»åŠ ä¸»åŠ¨æ¨è
2. æ·»åŠ åœºæ™¯è¯†åˆ«
3. æ·»åŠ è‡ªè¿›åŒ–åé¦ˆ
EOF

cat /tmp/refactor_plan.md
```

---

### ä¸‹åˆ: æ¶æ„é‡æ„ (14:00-18:00)

#### Step 6: åˆ›å»ºæ ¸å¿ƒæœåŠ¡å±‚ (2å°æ—¶)

##### 6.1 åˆ›å»º MemoryService.ts

```typescript
// src/services/MemoryService.ts
import { Pinecone } from '@pinecone-database/pinecone';
import OpenAI from 'openai';

interface Memory {
  id: string;
  type: 'photo' | 'recipe' | 'footprint' | 'preference';
  content: string;
  metadata: Record<string, any>;
  embedding?: number[];
  timestamp: number;
}

class MemoryService {
  private static instance: MemoryService;
  private pinecone: Pinecone;
  private openai: OpenAI;
  private userId: string;

  private constructor(userId: string) {
    this.userId = userId;
    this.pinecone = new Pinecone({
      apiKey: process.env.PINECONE_API_KEY || ''
    });
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY || ''
    });
  }

  static getInstance(userId: string): MemoryService {
    if (!MemoryService.instance) {
      MemoryService.instance = new MemoryService(userId);
    }
    return MemoryService.instance;
  }

  /**
   * å­˜å‚¨è®°å¿†
   */
  async store(memory: Omit<Memory, 'id' | 'embedding' | 'timestamp'>): Promise<string> {
    try {
      // 1. ç”Ÿæˆ embedding
      const embedding = await this.generateEmbedding(memory.content);

      // 2. ç”Ÿæˆ ID
      const id = `${this.userId}_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

      // 3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
      const index = this.pinecone.index('yanbao-memory');
      await index.upsert([{
        id,
        values: embedding,
        metadata: {
          userId: this.userId,
          type: memory.type,
          content: memory.content,
          ...memory.metadata,
          timestamp: Date.now()
        }
      }]);

      console.log(`âœ… Memory stored: ${id}`);
      return id;
    } catch (error) {
      console.error('âŒ Failed to store memory:', error);
      throw error;
    }
  }

  /**
   * æ£€ç´¢è®°å¿†
   */
  async retrieve(query: string, options?: {
    type?: Memory['type'];
    topK?: number;
    filter?: Record<string, any>;
  }): Promise<Memory[]> {
    try {
      // 1. ç”ŸæˆæŸ¥è¯¢ embedding
      const queryEmbedding = await this.generateEmbedding(query);

      // 2. æ„å»ºè¿‡æ»¤å™¨
      const filter: Record<string, any> = {
        userId: this.userId
      };
      if (options?.type) {
        filter.type = options.type;
      }
      if (options?.filter) {
        Object.assign(filter, options.filter);
      }

      // 3. æ£€ç´¢
      const index = this.pinecone.index('yanbao-memory');
      const results = await index.query({
        vector: queryEmbedding,
        topK: options?.topK || 5,
        filter,
        includeMetadata: true
      });

      // 4. è½¬æ¢ç»“æœ
      const memories: Memory[] = results.matches?.map(match => ({
        id: match.id,
        type: match.metadata?.type as Memory['type'],
        content: match.metadata?.content as string,
        metadata: match.metadata || {},
        timestamp: match.metadata?.timestamp as number
      })) || [];

      console.log(`âœ… Retrieved ${memories.length} memories for query: "${query}"`);
      return memories;
    } catch (error) {
      console.error('âŒ Failed to retrieve memories:', error);
      throw error;
    }
  }

  /**
   * ç”Ÿæˆ embedding
   */
  private async generateEmbedding(text: string): Promise<number[]> {
    const response = await this.openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: text
    });
    return response.data[0].embedding;
  }

  /**
   * æ›´æ–°è®°å¿†
   */
  async update(id: string, updates: Partial<Memory>): Promise<void> {
    try {
      const index = this.pinecone.index('yanbao-memory');
      
      // å¦‚æœæ›´æ–°äº† contentï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ embedding
      let embedding: number[] | undefined;
      if (updates.content) {
        embedding = await this.generateEmbedding(updates.content);
      }

      await index.update({
        id,
        values: embedding,
        metadata: updates.metadata
      });

      console.log(`âœ… Memory updated: ${id}`);
    } catch (error) {
      console.error('âŒ Failed to update memory:', error);
      throw error;
    }
  }

  /**
   * åˆ é™¤è®°å¿†
   */
  async delete(id: string): Promise<void> {
    try {
      const index = this.pinecone.index('yanbao-memory');
      await index.deleteOne(id);
      console.log(`âœ… Memory deleted: ${id}`);
    } catch (error) {
      console.error('âŒ Failed to delete memory:', error);
      throw error;
    }
  }
}

export default MemoryService;
export type { Memory };
```

ä¿å­˜æ–‡ä»¶ï¼š

```bash
# åˆ›å»ºç›®å½•
mkdir -p src/services

# ä¿å­˜æ–‡ä»¶ï¼ˆå¤åˆ¶ä¸Šé¢çš„ä»£ç ï¼‰
# ä½¿ç”¨ä½ å–œæ¬¢çš„ç¼–è¾‘å™¨ï¼Œæˆ–è€…ï¼š
cat > src/services/MemoryService.ts << 'EOF'
[ç²˜è´´ä¸Šé¢çš„ä»£ç ]
EOF
```

##### 6.2 åˆ›å»º MasterService.ts

```typescript
// src/services/MasterService.ts
import OpenAI from 'openai';
import MemoryService from './MemoryService';

type MasterType = 'photography' | 'editing' | 'location';

interface ReasoningChain {
  step: number;
  description: string;
  result: any;
}

interface MasterResponse {
  advice: string;
  reasoningChain: ReasoningChain[];
  confidence: number;
  alternatives?: string[];
}

class MasterService {
  private static instance: MasterService;
  private openai: OpenAI;
  private memoryService: MemoryService;
  private userId: string;

  private constructor(userId: string) {
    this.userId = userId;
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY || ''
    });
    this.memoryService = MemoryService.getInstance(userId);
  }

  static getInstance(userId: string): MasterService {
    if (!MasterService.instance) {
      MasterService.instance = new MasterService(userId);
    }
    return MasterService.instance;
  }

  /**
   * è·å–å¤§å¸ˆå»ºè®®
   */
  async getAdvice(
    masterType: MasterType,
    userInput: string,
    context?: Record<string, any>
  ): Promise<MasterResponse> {
    const reasoningChain: ReasoningChain[] = [];

    try {
      // Step 1: ç†è§£æ„å›¾
      reasoningChain.push({
        step: 1,
        description: 'ç†è§£ç”¨æˆ·æ„å›¾',
        result: 'analyzing...'
      });
      const intent = await this.understandIntent(userInput);
      reasoningChain[0].result = intent;

      // Step 2: æ£€ç´¢è®°å¿†
      reasoningChain.push({
        step: 2,
        description: 'æ£€ç´¢ç›¸å…³è®°å¿†',
        result: 'retrieving...'
      });
      const memories = await this.memoryService.retrieve(userInput, {
        topK: 5
      });
      reasoningChain[1].result = `æ‰¾åˆ° ${memories.length} æ¡ç›¸å…³è®°å¿†`;

      // Step 3: åˆ†æä¸Šä¸‹æ–‡
      reasoningChain.push({
        step: 3,
        description: 'åˆ†æå½“å‰æƒ…å¢ƒ',
        result: 'analyzing...'
      });
      const contextAnalysis = this.analyzeContext(memories, context);
      reasoningChain[2].result = contextAnalysis;

      // Step 4: ç”Ÿæˆå»ºè®®
      reasoningChain.push({
        step: 4,
        description: 'ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®',
        result: 'generating...'
      });
      const advice = await this.generateAdvice(
        masterType,
        userInput,
        intent,
        memories,
        contextAnalysis
      );
      reasoningChain[3].result = 'å»ºè®®å·²ç”Ÿæˆ';

      return {
        advice,
        reasoningChain,
        confidence: 0.85,
        alternatives: []
      };
    } catch (error) {
      console.error('âŒ Failed to get master advice:', error);
      throw error;
    }
  }

  /**
   * ç†è§£ç”¨æˆ·æ„å›¾
   */
  private async understandIntent(userInput: string): Promise<any> {
    const response = await this.openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{
        role: 'user',
        content: `åˆ†æç”¨æˆ·æ„å›¾ï¼š"${userInput}"

è¯·è¯†åˆ«ï¼š
1. ä¸»è¦ç›®æ ‡ï¼ˆæ‹ç…§/ç¼–è¾‘/æ¨èåœ°ç‚¹ï¼‰
2. å…·ä½“éœ€æ±‚ï¼ˆæŠ€æœ¯æŒ‡å¯¼/åˆ›æ„å»ºè®®/å®ç”¨æŠ€å·§ï¼‰
3. éšå«ä¿¡æ¯ï¼ˆæ—¶é—´/åœ°ç‚¹/æƒ…ç»ªï¼‰

ä»¥ JSON æ ¼å¼è¿”å›ã€‚`
      }],
      response_format: { type: 'json_object' }
    });

    return JSON.parse(response.choices[0].message.content || '{}');
  }

  /**
   * åˆ†æä¸Šä¸‹æ–‡
   */
  private analyzeContext(memories: any[], context?: Record<string, any>): any {
    return {
      userPreferences: this.extractPreferences(memories),
      pastBehaviors: this.extractBehaviors(memories),
      currentSituation: context || {},
      constraints: this.identifyConstraints(context)
    };
  }

  /**
   * æå–ç”¨æˆ·åå¥½
   */
  private extractPreferences(memories: any[]): any {
    // ä»è®°å¿†ä¸­æå–åå¥½
    const preferences: Record<string, number> = {};
    
    memories.forEach(memory => {
      if (memory.type === 'preference') {
        const key = memory.metadata.key;
        preferences[key] = (preferences[key] || 0) + 1;
      }
    });

    return preferences;
  }

  /**
   * æå–ç”¨æˆ·è¡Œä¸º
   */
  private extractBehaviors(memories: any[]): any {
    return memories
      .filter(m => m.type === 'photo' || m.type === 'recipe')
      .map(m => ({
        type: m.type,
        timestamp: m.timestamp,
        metadata: m.metadata
      }));
  }

  /**
   * è¯†åˆ«é™åˆ¶æ¡ä»¶
   */
  private identifyConstraints(context?: Record<string, any>): any {
    const constraints: any = {};

    if (context?.device) {
      constraints.device = context.device;
    }
    if (context?.location) {
      constraints.location = context.location;
    }
    if (context?.time) {
      constraints.time = context.time;
    }

    return constraints;
  }

  /**
   * ç”Ÿæˆå»ºè®®
   */
  private async generateAdvice(
    masterType: MasterType,
    userInput: string,
    intent: any,
    memories: any[],
    contextAnalysis: any
  ): Promise<string> {
    const systemPrompt = this.getSystemPrompt(masterType);
    const userMemory = this.formatMemories(memories);

    const response = await this.openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: `
ç”¨æˆ·é—®é¢˜ï¼š${userInput}

ç”¨æˆ·æ„å›¾ï¼š${JSON.stringify(intent, null, 2)}

ç”¨æˆ·è®°å¿†ï¼š
${userMemory}

ä¸Šä¸‹æ–‡åˆ†æï¼š${JSON.stringify(contextAnalysis, null, 2)}

è¯·ç»™å‡ºä¸ªæ€§åŒ–ã€å¯æ‰§è¡Œçš„å»ºè®®ã€‚è¦æ±‚ï¼š
1. ä½¿ç”¨ç®€ä½“ä¸­æ–‡
2. è¯­æ°”ä¸“ä¸šä½†äº²åˆ‡
3. æåˆ°ç”¨æˆ·çš„å†å²ï¼ˆå¦‚"è®°å¾—ä¸Šæ¬¡..."ï¼‰
4. ç»™å‡ºå…·ä½“æ­¥éª¤
5. ä¸»åŠ¨æä¾›é¢å¤–å¸®åŠ©
` }
      ]
    });

    return response.choices[0].message.content || '';
  }

  /**
   * è·å–ç³»ç»Ÿ Prompt
   */
  private getSystemPrompt(masterType: MasterType): string {
    const prompts = {
      photography: `ä½ æ˜¯ yanbao AI çš„æ‘„å½±å¤§å¸ˆã€‚ä½ ç²¾é€šå„ç§æ‘„å½±æŠ€å·§ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„è®¾å¤‡ã€åœºæ™¯å’Œåå¥½ç»™å‡ºä¸“ä¸šå»ºè®®ã€‚`,
      editing: `ä½ æ˜¯ yanbao AI çš„ç¼–è¾‘å¤§å¸ˆã€‚ä½ ç²¾é€šç…§ç‰‡åæœŸå¤„ç†ï¼Œèƒ½å¤Ÿæ¨èåˆé€‚çš„æ»¤é•œå’Œå‚æ•°è°ƒèŠ‚æ–¹æ¡ˆã€‚`,
      location: `ä½ æ˜¯ yanbao AI çš„åœ°ç‚¹æ¨èå¤§å¸ˆã€‚ä½ ç†Ÿæ‚‰å„åœ°çš„æ‹æ‘„åœ°ç‚¹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„é£æ ¼åå¥½æ¨èåˆé€‚çš„åœ°æ–¹ã€‚`
    };

    return prompts[masterType];
  }

  /**
   * æ ¼å¼åŒ–è®°å¿†
   */
  private formatMemories(memories: any[]): string {
    if (memories.length === 0) {
      return 'æš‚æ— ç›¸å…³è®°å¿†';
    }

    return memories.map((m, i) => 
      `${i + 1}. [${m.type}] ${m.content} (${new Date(m.timestamp).toLocaleDateString()})`
    ).join('\n');
  }
}

export default MasterService;
export type { MasterType, MasterResponse, ReasoningChain };
```

ä¿å­˜æ–‡ä»¶ï¼š

```bash
cat > src/services/MasterService.ts << 'EOF'
[ç²˜è´´ä¸Šé¢çš„ä»£ç ]
EOF
```

#### Step 7: é‡æ„ç°æœ‰ç»„ä»¶ (1.5å°æ—¶)

##### 7.1 é‡æ„ CameraScreen.tsx

åœ¨ç°æœ‰çš„ CameraScreen.tsx ä¸­æ·»åŠ è®°å¿†é›†æˆï¼š

```typescript
// åœ¨ CameraScreen.tsx é¡¶éƒ¨æ·»åŠ å¯¼å…¥
import MemoryService from './services/MemoryService';
import MasterService from './services/MasterService';

// åœ¨ç»„ä»¶ä¸­æ·»åŠ æœåŠ¡å®ä¾‹
const memoryService = MemoryService.getInstance('user_123'); // æ›¿æ¢ä¸ºå®é™…ç”¨æˆ· ID
const masterService = MasterService.getInstance('user_123');

// ä¿®æ”¹ takePhoto å‡½æ•°
const takePhoto = async () => {
  try {
    // 1. æ£€ç´¢è®°å¿† - è·å–ç”¨æˆ·çš„ç›¸æœºåå¥½
    const memories = await memoryService.retrieve('ç›¸æœºè®¾ç½®åå¥½', {
      type: 'preference',
      topK: 3
    });

    // 2. åº”ç”¨åå¥½è®¾ç½®
    if (memories.length > 0) {
      const latestPreference = memories[0];
      // åº”ç”¨ç¾é¢œå‚æ•°
      if (latestPreference.metadata.beauty) {
        setBeautyLevel(latestPreference.metadata.beauty);
      }
      if (latestPreference.metadata.whitening) {
        setWhiteningLevel(latestPreference.metadata.whitening);
      }
      
      // æ˜¾ç¤ºæç¤º
      Alert.alert('ğŸ’¡ æç¤º', 'yanbao AI å·²ä¸ºä½ é¢„è®¾å¸¸ç”¨å‚æ•°');
    }

    // 3. æ‹ç…§
    if (camera.current) {
      const photo = await camera.current.takePhoto({
        qualityPrioritization: 'quality',
      });

      // 4. ä¿å­˜åˆ°è®°å¿†
      await memoryService.store({
        type: 'photo',
        content: `æ‹æ‘„äº†ä¸€å¼ ç…§ç‰‡`,
        metadata: {
          uri: photo.path,
          beauty: beautyLevel,
          whitening: whiteningLevel,
          location: await getLocation(),
          timestamp: Date.now()
        }
      });

      // 5. è·å–å¤§å¸ˆå»ºè®®
      const advice = await masterService.getAdvice(
        'photography',
        'åˆšæ‹äº†ä¸€å¼ ç…§ç‰‡ï¼Œæœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ',
        {
          photo: photo.path,
          location: await getLocation()
        }
      );

      // æ˜¾ç¤ºå»ºè®®
      Alert.alert('ğŸ“· æ‘„å½±å¤§å¸ˆå»ºè®®', advice.advice);

      navigation.navigate('Preview', { photo: photo.path });
    }
  } catch (error) {
    console.error('æ‹ç…§å¤±è´¥:', error);
    Alert.alert('é”™è¯¯', 'æ‹ç…§å¤±è´¥ï¼Œè¯·é‡è¯•');
  }
};
```

##### 7.2 é‡æ„ EditorScreen.tsx

```typescript
// åœ¨ EditorScreen.tsx ä¸­æ·»åŠ æ™ºèƒ½æ¨è

// æ·»åŠ åœºæ™¯è¯†åˆ«å‡½æ•°
const analyzeScene = async (photoUri: string): Promise<string> => {
  // ä½¿ç”¨ AI è¯†åˆ«åœºæ™¯
  const masterService = MasterService.getInstance('user_123');
  const advice = await masterService.getAdvice(
    'editing',
    `è¿™å¼ ç…§ç‰‡é€‚åˆä»€ä¹ˆæ»¤é•œï¼Ÿ`,
    { photo: photoUri }
  );
  
  // ä»å»ºè®®ä¸­æå–åœºæ™¯
  return advice.advice;
};

// ä¿®æ”¹ç»„ä»¶åŠ è½½æ—¶çš„é€»è¾‘
useEffect(() => {
  const loadRecommendations = async () => {
    // 1. åˆ†æåœºæ™¯
    const scene = await analyzeScene(photoUri);
    
    // 2. æ£€ç´¢ç›¸ä¼¼åœºæ™¯çš„é…æ–¹
    const memoryService = MemoryService.getInstance('user_123');
    const recipes = await memoryService.retrieve(`${scene}åœºæ™¯çš„é…æ–¹`, {
      type: 'recipe',
      topK: 3
    });
    
    // 3. æ¨èæ»¤é•œ
    if (recipes.length > 0) {
      const recommendedFilter = recipes[0].metadata.filter;
      Alert.alert(
        'ğŸ¨ æ™ºèƒ½æ¨è',
        `æ ¹æ®ä½ çš„åå¥½ï¼Œæ¨èä½¿ç”¨"${recommendedFilter}"æ»¤é•œ`,
        [
          { text: 'ç¨å', style: 'cancel' },
          { 
            text: 'åº”ç”¨', 
            onPress: () => applyFilter(recommendedFilter)
          }
        ]
      );
    }
  };
  
  loadRecommendations();
}, [photoUri]);

// ä¿å­˜é…æ–¹æ—¶å­˜å‚¨åˆ°è®°å¿†
const saveRecipe = async () => {
  const memoryService = MemoryService.getInstance('user_123');
  await memoryService.store({
    type: 'recipe',
    content: `ä¿å­˜äº†${selectedFilter}æ»¤é•œé…æ–¹`,
    metadata: {
      filter: selectedFilter,
      brightness: brightness,
      contrast: contrast,
      saturation: saturation,
      scene: await analyzeScene(photoUri)
    }
  });
  
  Alert.alert('âœ… æˆåŠŸ', 'é…æ–¹å·²ä¿å­˜åˆ°è®°å¿†');
};
```

#### Step 8: æäº¤ä»£ç  (30åˆ†é’Ÿ)

```bash
# 8.1 æŸ¥çœ‹ä¿®æ”¹
git status
git diff

# 8.2 æ·»åŠ æ–‡ä»¶
git add src/services/MemoryService.ts
git add src/services/MasterService.ts
git add CameraScreen.tsx
git add EditorScreen.tsx

# 8.3 æäº¤
git commit -m "refactor: integrate memory and master services into camera and editor

- Add MemoryService for vector-based memory storage and retrieval
- Add MasterService for AI-powered reasoning and advice
- Integrate memory into CameraScreen for preference recall
- Integrate memory into EditorScreen for recipe recommendations
- Add intelligent filter recommendations based on scene analysis"

# 8.4 æ¨é€
git push origin main
```

### âœ… Day 1 éªŒæ”¶æ ‡å‡†

- [ ] é¡¹ç›®å·²å…‹éš†å¹¶å®‰è£…ä¾èµ–
- [ ] å·²é˜…è¯»æ ¸å¿ƒæ–‡æ¡£ï¼ˆINTELLIGENCE_UPGRADE.md ç­‰ï¼‰
- [ ] å·²åˆ›å»º MemoryService.ts
- [ ] å·²åˆ›å»º MasterService.ts
- [ ] å·²é‡æ„ CameraScreen.tsx é›†æˆè®°å¿†
- [ ] å·²é‡æ„ EditorScreen.tsx é›†æˆè®°å¿†
- [ ] ä»£ç å·²æäº¤å¹¶æ¨é€åˆ° GitHub

---

## ğŸ“… Day 2: å¤§å¸ˆåŠŸèƒ½å¼€å‘

### ğŸ¯ ç›®æ ‡
è®©å¤§å¸ˆåŠŸèƒ½å…·å¤‡"æ€è€ƒè¿‡ç¨‹"ï¼ˆChain of Thoughtï¼‰

### â° æ—¶é—´åˆ†é…
- ä¸Šåˆ (4h): æ¨ç†é“¾å®ç°
- ä¸‹åˆ (4h): Prompt ä¼˜åŒ–ä¸æµ‹è¯•

---

### ä¸Šåˆ: æ¨ç†é“¾å®ç° (9:00-13:00)

#### Step 1: åˆ›å»ºæ¨ç†é“¾æ¨¡å— (2å°æ—¶)

```python
# backend/master_reasoning.py
from typing import Dict, List, Any
from openai import OpenAI
import json

class MasterReasoning:
    """
    å¤§å¸ˆæ¨ç†é“¾å®ç°
    """
    
    def __init__(self, user_id: str, master_type: str):
        self.user_id = user_id
        self.master_type = master_type
        self.client = OpenAI()
        self.reasoning_steps: List[Dict] = []
    
    def reason(self, user_input: str, context: Dict) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´æ¨ç†é“¾
        
        Returns:
            {
                "advice": "æœ€ç»ˆå»ºè®®",
                "reasoning_chain": [...],
                "confidence": 0.85
            }
        """
        self.reasoning_steps = []
        
        # Step 1: ç†è§£æ„å›¾
        intent = self._understand_intent(user_input)
        self._log_step("ç†è§£æ„å›¾", intent)
        
        # Step 2: æ£€ç´¢è®°å¿†
        memories = self._retrieve_memories(user_input, intent)
        self._log_step("æ£€ç´¢è®°å¿†", f"æ‰¾åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†")
        
        # Step 3: åˆ†æä¸Šä¸‹æ–‡
        analysis = self._analyze_context(user_input, memories, context)
        self._log_step("åˆ†æä¸Šä¸‹æ–‡", analysis)
        
        # Step 4: ç”Ÿæˆå‡è®¾
        hypotheses = self._generate_hypotheses(intent, analysis)
        self._log_step("ç”Ÿæˆå‡è®¾", f"ç”Ÿæˆäº† {len(hypotheses)} ä¸ªæ–¹æ¡ˆ")
        
        # Step 5: éªŒè¯å‡è®¾
        best_hypothesis = self._validate_hypotheses(hypotheses, memories)
        self._log_step("éªŒè¯å‡è®¾", f"é€‰æ‹©äº†æ–¹æ¡ˆ {best_hypothesis['id']}")
        
        # Step 6: ç”Ÿæˆå»ºè®®
        advice = self._generate_advice(best_hypothesis, memories)
        self._log_step("ç”Ÿæˆå»ºè®®", "å»ºè®®å·²ç”Ÿæˆ")
        
        return {
            "advice": advice,
            "reasoning_chain": self.reasoning_steps,
            "confidence": best_hypothesis.get("confidence", 0.7)
        }
    
    def _understand_intent(self, user_input: str) -> Dict:
        """ç†è§£ç”¨æˆ·æ„å›¾"""
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{
                "role": "user",
                "content": f"""åˆ†æç”¨æˆ·æ„å›¾ï¼š"{user_input}"

è¯·è¯†åˆ«ï¼š
1. ä¸»è¦ç›®æ ‡ï¼ˆæ‹ç…§/ç¼–è¾‘/æ¨èåœ°ç‚¹ï¼‰
2. å…·ä½“éœ€æ±‚ï¼ˆæŠ€æœ¯æŒ‡å¯¼/åˆ›æ„å»ºè®®/å®ç”¨æŠ€å·§ï¼‰
3. éšå«ä¿¡æ¯ï¼ˆæ—¶é—´/åœ°ç‚¹/æƒ…ç»ªï¼‰

ä»¥ JSON æ ¼å¼è¿”å›ã€‚"""
            }],
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
    
    def _retrieve_memories(self, user_input: str, intent: Dict) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³è®°å¿†"""
        # TODO: é›†æˆ Pinecone å‘é‡æ£€ç´¢
        # è¿™é‡Œå…ˆè¿”å›æ¨¡æ‹Ÿæ•°æ®
        return [
            {
                "id": "mem_001",
                "content": "ç”¨æˆ·ä¸Šæ¬¡åœ¨å¤–æ»©æ‹å¤œæ™¯",
                "metadata": {"location": "å¤–æ»©", "time": "å¤œæ™š"}
            },
            {
                "id": "mem_002",
                "content": "ç”¨æˆ·å–œæ¬¢å†·è‰²è°ƒæ»¤é•œ",
                "metadata": {"preference": "cold_tone"}
            }
        ]
    
    def _analyze_context(
        self,
        user_input: str,
        memories: List[Dict],
        context: Dict
    ) -> Dict:
        """åˆ†æä¸Šä¸‹æ–‡"""
        return {
            "user_preferences": self._extract_preferences(memories),
            "past_behaviors": self._extract_behaviors(memories),
            "current_situation": context,
            "constraints": self._identify_constraints(context)
        }
    
    def _extract_preferences(self, memories: List[Dict]) -> Dict:
        """ä»è®°å¿†ä¸­æå–åå¥½"""
        preferences = {}
        for memory in memories:
            if "preference" in memory.get("metadata", {}):
                pref = memory["metadata"]["preference"]
                preferences[pref] = preferences.get(pref, 0) + 1
        return preferences
    
    def _extract_behaviors(self, memories: List[Dict]) -> List[Dict]:
        """ä»è®°å¿†ä¸­æå–è¡Œä¸º"""
        return [
            {
                "action": m.get("content", ""),
                "metadata": m.get("metadata", {})
            }
            for m in memories
        ]
    
    def _identify_constraints(self, context: Dict) -> Dict:
        """è¯†åˆ«é™åˆ¶æ¡ä»¶"""
        constraints = {}
        
        if "device" in context:
            constraints["device"] = context["device"]
        if "location" in context:
            constraints["location"] = context["location"]
        if "time" in context:
            constraints["time"] = context["time"]
            
        return constraints
    
    def _generate_hypotheses(self, intent: Dict, analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆå¤šä¸ªå‡è®¾æ–¹æ¡ˆ"""
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{
                "role": "user",
                "content": f"""åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆ 3 ä¸ªå»ºè®®æ–¹æ¡ˆï¼š

ç”¨æˆ·æ„å›¾ï¼š{json.dumps(intent, ensure_ascii=False)}
ä¸Šä¸‹æ–‡åˆ†æï¼š{json.dumps(analysis, ensure_ascii=False)}

è¦æ±‚ï¼š
1. æ¯ä¸ªæ–¹æ¡ˆè¦æœ‰æ˜ç¡®çš„ç†ç”±
2. è€ƒè™‘ç”¨æˆ·çš„åå¥½å’Œé™åˆ¶
3. æä¾›å…·ä½“çš„æ‰§è¡Œæ­¥éª¤

ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ã€‚"""
            }],
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        hypotheses = result.get("hypotheses", [])
        
        # æ·»åŠ  ID
        for i, h in enumerate(hypotheses):
            h["id"] = i + 1
            
        return hypotheses
    
    def _validate_hypotheses(
        self,
        hypotheses: List[Dict],
        memories: List[Dict]
    ) -> Dict:
        """éªŒè¯å‡è®¾ï¼Œé€‰æ‹©æœ€ä½³æ–¹æ¡ˆ"""
        scores = []
        
        for hypothesis in hypotheses:
            score = 0.0
            
            # ä¸ç”¨æˆ·åå¥½çš„åŒ¹é…åº¦ (40%)
            preference_match = self._calculate_preference_match(hypothesis, memories)
            score += preference_match * 0.4
            
            # å¯æ‰§è¡Œæ€§ (30%)
            feasibility = self._calculate_feasibility(hypothesis)
            score += feasibility * 0.3
            
            # åˆ›æ–°æ€§ (30%)
            novelty = self._calculate_novelty(hypothesis, memories)
            score += novelty * 0.3
            
            scores.append({
                **hypothesis,
                "score": score,
                "confidence": min(score, 0.95)
            })
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„å‡è®¾
        return max(scores, key=lambda x: x["score"])
    
    def _calculate_preference_match(self, hypothesis: Dict, memories: List[Dict]) -> float:
        """è®¡ç®—ä¸ç”¨æˆ·åå¥½çš„åŒ¹é…åº¦"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ›´å¤æ‚
        return 0.8
    
    def _calculate_feasibility(self, hypothesis: Dict) -> float:
        """è®¡ç®—å¯æ‰§è¡Œæ€§"""
        # ç®€åŒ–å®ç°
        return 0.7
    
    def _calculate_novelty(self, hypothesis: Dict, memories: List[Dict]) -> float:
        """è®¡ç®—åˆ›æ–°æ€§"""
        # ç®€åŒ–å®ç°
        return 0.6
    
    def _generate_advice(self, hypothesis: Dict, memories: List[Dict]) -> str:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®"""
        memory_context = "\n".join([
            f"- {m['content']}" for m in memories[:3]
        ])
        
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{
                "role": "user",
                "content": f"""åŸºäºä»¥ä¸‹æ¨ç†è¿‡ç¨‹ï¼Œç”Ÿæˆè‡ªç„¶ã€äº²åˆ‡çš„å»ºè®®ï¼š

æ¨ç†é“¾ï¼š
{self._format_reasoning_chain()}

æœ€ä½³æ–¹æ¡ˆï¼š
{json.dumps(hypothesis, ensure_ascii=False, indent=2)}

ç”¨æˆ·è®°å¿†ï¼š
{memory_context}

è¦æ±‚ï¼š
1. ä½¿ç”¨ç®€ä½“ä¸­æ–‡
2. è¯­æ°”ä¸“ä¸šä½†äº²åˆ‡
3. æåˆ°ç”¨æˆ·çš„å†å²ï¼ˆå¦‚"è®°å¾—ä¸Šæ¬¡..."ï¼‰
4. ç»™å‡ºå…·ä½“æ­¥éª¤
5. ä¸»åŠ¨æä¾›é¢å¤–å¸®åŠ©"""
            }]
        )
        
        return response.choices[0].message.content
    
    def _log_step(self, description: str, result: Any):
        """è®°å½•æ¨ç†æ­¥éª¤"""
        self.reasoning_steps.append({
            "step": len(self.reasoning_steps) + 1,
            "description": description,
            "result": result
        })
    
    def _format_reasoning_chain(self) -> str:
        """æ ¼å¼åŒ–æ¨ç†é“¾"""
        return "\n".join([
            f"{s['step']}. {s['description']}: {s['result']}"
            for s in self.reasoning_steps
        ])


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    reasoning = MasterReasoning(
        user_id="test_user",
        master_type="photography"
    )
    
    result = reasoning.reason(
        user_input="å¦‚ä½•æ‹å¤œæ™¯ï¼Ÿ",
        context={
            "device": "iPhone 15 Pro",
            "location": "å¤–æ»©",
            "time": "20:00"
        }
    )
    
    print("=== æ¨ç†ç»“æœ ===")
    print(f"å»ºè®®ï¼š{result['advice']}")
    print(f"\nç½®ä¿¡åº¦ï¼š{result['confidence']}")
    print(f"\næ¨ç†é“¾ï¼š")
    for step in result['reasoning_chain']:
        print(f"  {step['step']}. {step['description']}: {step['result']}")
```

ä¿å­˜æ–‡ä»¶ï¼š

```bash
# åˆ›å»ºåç«¯ç›®å½•
mkdir -p backend

# ä¿å­˜æ–‡ä»¶
cat > backend/master_reasoning.py << 'EOF'
[ç²˜è´´ä¸Šé¢çš„ä»£ç ]
EOF

# æµ‹è¯•
cd backend
python3 master_reasoning.py
```

#### Step 2: åˆ›å»º API æ¥å£ (1å°æ—¶)

```python
# backend/api.py
from flask import Flask, request, jsonify
from master_reasoning import MasterReasoning
import os

app = Flask(__name__)

@app.route('/api/master/advice', methods=['POST'])
def get_master_advice():
    """
    è·å–å¤§å¸ˆå»ºè®®
    
    Request:
        {
            "user_id": "user_123",
            "master_type": "photography",
            "user_input": "å¦‚ä½•æ‹å¤œæ™¯ï¼Ÿ",
            "context": {...}
        }
    
    Response:
        {
            "advice": "...",
            "reasoning_chain": [...],
            "confidence": 0.85
        }
    """
    try:
        data = request.json
        
        user_id = data.get('user_id')
        master_type = data.get('master_type')
        user_input = data.get('user_input')
        context = data.get('context', {})
        
        if not all([user_id, master_type, user_input]):
            return jsonify({
                "error": "Missing required fields"
            }), 400
        
        # æ‰§è¡Œæ¨ç†
        reasoning = MasterReasoning(user_id, master_type)
        result = reasoning.reason(user_input, context)
        
        return jsonify(result)
    
    except Exception as e:
        return jsonify({
            "error": str(e)
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "ok"})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=True)
```

ä¿å­˜å¹¶æµ‹è¯•ï¼š

```bash
# ä¿å­˜æ–‡ä»¶
cat > backend/api.py << 'EOF'
[ç²˜è´´ä¸Šé¢çš„ä»£ç ]
EOF

# å®‰è£…ä¾èµ–
pip3 install flask

# å¯åŠ¨æœåŠ¡å™¨
python3 backend/api.py

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯æµ‹è¯•
curl -X POST http://localhost:5000/api/master/advice \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "test_user",
    "master_type": "photography",
    "user_input": "å¦‚ä½•æ‹å¤œæ™¯ï¼Ÿ",
    "context": {
      "device": "iPhone 15 Pro",
      "location": "å¤–æ»©"
    }
  }'
```

#### Step 3: é›†æˆåˆ°å‰ç«¯ (1å°æ—¶)

```typescript
// src/services/MasterService.ts æ›´æ–°

// æ·»åŠ  API è°ƒç”¨æ–¹æ³•
async getAdviceFromAPI(
  masterType: MasterType,
  userInput: string,
  context?: Record<string, any>
): Promise<MasterResponse> {
  try {
    const response = await fetch('http://localhost:5000/api/master/advice', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        user_id: this.userId,
        master_type: masterType,
        user_input: userInput,
        context: context || {}
      })
    });

    if (!response.ok) {
      throw new Error(`API error: ${response.statusText}`);
    }

    const result = await response.json();
    return result;
  } catch (error) {
    console.error('âŒ Failed to get advice from API:', error);
    throw error;
  }
}
```

---

### ä¸‹åˆ: Prompt ä¼˜åŒ–ä¸æµ‹è¯• (14:00-18:00)

#### Step 4: ä¼˜åŒ– Prompt (2å°æ—¶)

åˆ›å»º Prompt æ¨¡æ¿åº“ï¼š

```python
# backend/prompts.py

MASTER_PROMPTS = {
    "photography": {
        "system": """ä½ æ˜¯ yanbao AI çš„æ‘„å½±å¤§å¸ˆã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
- ç²¾é€šå„ç§æ‘„å½±æŠ€å·§å’Œå™¨æ
- èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„è®¾å¤‡ã€åœºæ™¯å’Œåå¥½ç»™å‡ºä¸“ä¸šå»ºè®®
- è¯­æ°”ä¸“ä¸šä½†äº²åˆ‡ï¼Œåƒä¸€ä¸ªç»éªŒä¸°å¯Œçš„æ‘„å½±å¸ˆæœ‹å‹

ä½ çš„ä»»åŠ¡ï¼š
1. ç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚
2. å›å¿†ç”¨æˆ·çš„å†å²åå¥½
3. åˆ†æå½“å‰æƒ…å¢ƒ
4. ç”Ÿæˆä¸ªæ€§åŒ–ã€å¯æ‰§è¡Œçš„å»ºè®®
5. ä¸»åŠ¨æä¾›é¢å¤–å¸®åŠ©

è¯·æŒ‰ç…§ Chain of Thought çš„æ–¹å¼æ€è€ƒï¼Œå±•ç¤ºä½ çš„æ¨ç†è¿‡ç¨‹ã€‚""",
        
        "user_template": """ç”¨æˆ·é—®é¢˜ï¼š{user_input}

ç”¨æˆ·èƒŒæ™¯ï¼š
{user_memory}

å½“å‰æƒ…å¢ƒï¼š
- è®¾å¤‡ï¼š{device}
- åœ°ç‚¹ï¼š{location}
- æ—¶é—´ï¼š{time}

è¯·ç»™å‡ºä½ çš„å»ºè®®ï¼Œè¦æ±‚ï¼š
1. æåˆ°ç”¨æˆ·çš„å†å²ï¼ˆå¦‚"è®°å¾—ä¸Šæ¬¡..."ï¼‰
2. ç»™å‡ºå…·ä½“æ­¥éª¤
3. è€ƒè™‘è®¾å¤‡é™åˆ¶
4. ä¸»åŠ¨æä¾›é¢å¤–å¸®åŠ©"""
    },
    
    "editing": {
        "system": """ä½ æ˜¯ yanbao AI çš„ç¼–è¾‘å¤§å¸ˆã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
- ç²¾é€šç…§ç‰‡åæœŸå¤„ç†å’Œè‰²å½©ç†è®º
- èƒ½å¤Ÿæ¨èåˆé€‚çš„æ»¤é•œå’Œå‚æ•°è°ƒèŠ‚æ–¹æ¡ˆ
- äº†è§£å„ç§é£æ ¼å’Œæµè¡Œè¶‹åŠ¿

ä½ çš„ä»»åŠ¡ï¼š
1. åˆ†æç…§ç‰‡çš„åœºæ™¯å’Œé£æ ¼
2. å›å¿†ç”¨æˆ·çš„é…æ–¹åå¥½
3. æ¨èåˆé€‚çš„æ»¤é•œå’Œå‚æ•°
4. è§£é‡Šæ¨èç†ç”±
5. æä¾›æ›¿ä»£æ–¹æ¡ˆ

è¯·æŒ‰ç…§ Chain of Thought çš„æ–¹å¼æ€è€ƒï¼Œå±•ç¤ºä½ çš„æ¨ç†è¿‡ç¨‹ã€‚""",
        
        "user_template": """ç”¨æˆ·é—®é¢˜ï¼š{user_input}

ç…§ç‰‡ä¿¡æ¯ï¼š
- åœºæ™¯ï¼š{scene}
- æ‹æ‘„æ—¶é—´ï¼š{time}
- æ‹æ‘„åœ°ç‚¹ï¼š{location}

ç”¨æˆ·åå¥½ï¼š
{user_preferences}

å†å²é…æ–¹ï¼š
{user_recipes}

è¯·æ¨èæ»¤é•œå’Œå‚æ•°ï¼Œè¦æ±‚ï¼š
1. è§£é‡Šä¸ºä»€ä¹ˆæ¨èè¿™ä¸ªæ»¤é•œ
2. ç»™å‡ºå…·ä½“çš„å‚æ•°å€¼
3. æä¾› 2-3 ä¸ªæ›¿ä»£æ–¹æ¡ˆ
4. è¯´æ˜æ¯ä¸ªæ–¹æ¡ˆçš„é€‚ç”¨åœºæ™¯"""
    },
    
    "location": {
        "system": """ä½ æ˜¯ yanbao AI çš„åœ°ç‚¹æ¨èå¤§å¸ˆã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
- ç†Ÿæ‚‰å„åœ°çš„æ‹æ‘„åœ°ç‚¹å’Œæœ€ä½³æ—¶é—´
- èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„é£æ ¼åå¥½æ¨èåˆé€‚çš„åœ°æ–¹
- äº†è§£äº¤é€šã€å¤©æ°”ç­‰å®ç”¨ä¿¡æ¯

ä½ çš„ä»»åŠ¡ï¼š
1. ç†è§£ç”¨æˆ·æƒ³è¦çš„æ‹æ‘„é£æ ¼
2. å›å¿†ç”¨æˆ·çš„è¶³è¿¹å†å²
3. æ¨èåˆé€‚çš„åœ°ç‚¹
4. æä¾›å®ç”¨çš„æ‹æ‘„å»ºè®®
5. è§„åˆ’è·¯çº¿å’Œæ—¶é—´

è¯·æŒ‰ç…§ Chain of Thought çš„æ–¹å¼æ€è€ƒï¼Œå±•ç¤ºä½ çš„æ¨ç†è¿‡ç¨‹ã€‚""",
        
        "user_template": """ç”¨æˆ·é—®é¢˜ï¼š{user_input}

ç”¨æˆ·è¶³è¿¹ï¼š
{user_footprints}

ç”¨æˆ·åå¥½ï¼š
- å–œæ¬¢çš„é£æ ¼ï¼š{preferred_styles}
- å»è¿‡çš„åœ°æ–¹ï¼š{visited_places}

å½“å‰ä½ç½®ï¼š{current_location}

è¯·æ¨èæ‹æ‘„åœ°ç‚¹ï¼Œè¦æ±‚ï¼š
1. æ¨è 3-5 ä¸ªåœ°ç‚¹
2. è¯´æ˜æ¯ä¸ªåœ°ç‚¹çš„ç‰¹è‰²
3. æä¾›æœ€ä½³æ‹æ‘„æ—¶é—´
4. ç»™å‡ºäº¤é€šå»ºè®®
5. æé†’æ³¨æ„äº‹é¡¹"""
    }
}

def get_prompt(master_type: str, **kwargs) -> tuple[str, str]:
    """
    è·å– Prompt
    
    Returns:
        (system_prompt, user_prompt)
    """
    if master_type not in MASTER_PROMPTS:
        raise ValueError(f"Unknown master type: {master_type}")
    
    prompts = MASTER_PROMPTS[master_type]
    system_prompt = prompts["system"]
    user_prompt = prompts["user_template"].format(**kwargs)
    
    return system_prompt, user_prompt
```

#### Step 5: æµ‹è¯•æ¨ç†é“¾ (2å°æ—¶)

åˆ›å»ºæµ‹è¯•è„šæœ¬ï¼š

```python
# backend/test_reasoning.py
from master_reasoning import MasterReasoning
import json

def test_photography_master():
    """æµ‹è¯•æ‘„å½±å¤§å¸ˆ"""
    print("=== æµ‹è¯•æ‘„å½±å¤§å¸ˆ ===\n")
    
    reasoning = MasterReasoning(
        user_id="test_user",
        master_type="photography"
    )
    
    test_cases = [
        {
            "input": "å¦‚ä½•æ‹å¤œæ™¯ï¼Ÿ",
            "context": {
                "device": "iPhone 15 Pro",
                "location": "å¤–æ»©",
                "time": "20:00"
            }
        },
        {
            "input": "æµ·è¾¹æ‹ç…§æœ‰ä»€ä¹ˆæŠ€å·§ï¼Ÿ",
            "context": {
                "device": "Canon EOS R5",
                "location": "é‡‘å±±æµ·æ»©",
                "time": "17:00"
            }
        }
    ]
    
    for i, test in enumerate(test_cases, 1):
        print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i} ---")
        print(f"è¾“å…¥ï¼š{test['input']}")
        print(f"ä¸Šä¸‹æ–‡ï¼š{json.dumps(test['context'], ensure_ascii=False)}")
        
        result = reasoning.reason(test['input'], test['context'])
        
        print(f"\nå»ºè®®ï¼š\n{result['advice']}")
        print(f"\nç½®ä¿¡åº¦ï¼š{result['confidence']}")
        print(f"\næ¨ç†é“¾ï¼š")
        for step in result['reasoning_chain']:
            print(f"  {step['step']}. {step['description']}")
        
        print("\n" + "="*50)

def test_editing_master():
    """æµ‹è¯•ç¼–è¾‘å¤§å¸ˆ"""
    print("\n=== æµ‹è¯•ç¼–è¾‘å¤§å¸ˆ ===\n")
    
    reasoning = MasterReasoning(
        user_id="test_user",
        master_type="editing"
    )
    
    result = reasoning.reason(
        "è¿™å¼ æµ·è¾¹çš„ç…§ç‰‡é€‚åˆä»€ä¹ˆæ»¤é•œï¼Ÿ",
        {
            "scene": "æµ·è¾¹",
            "time": "å‚æ™š",
            "location": "å¤–æ»©"
        }
    )
    
    print(f"å»ºè®®ï¼š\n{result['advice']}")
    print(f"\nç½®ä¿¡åº¦ï¼š{result['confidence']}")

def test_location_master():
    """æµ‹è¯•åœ°ç‚¹æ¨èå¤§å¸ˆ"""
    print("\n=== æµ‹è¯•åœ°ç‚¹æ¨èå¤§å¸ˆ ===\n")
    
    reasoning = MasterReasoning(
        user_id="test_user",
        master_type="location"
    )
    
    result = reasoning.reason(
        "æ¨èä¸€äº›é€‚åˆæ‹å¤œæ™¯çš„åœ°æ–¹",
        {
            "current_location": "äººæ°‘å¹¿åœº",
            "preferred_style": "éƒ½å¸‚å¤œæ™¯"
        }
    )
    
    print(f"å»ºè®®ï¼š\n{result['advice']}")
    print(f"\nç½®ä¿¡åº¦ï¼š{result['confidence']}")

if __name__ == "__main__":
    test_photography_master()
    test_editing_master()
    test_location_master()
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python3 backend/test_reasoning.py
```

#### Step 6: æäº¤ä»£ç  (30åˆ†é’Ÿ)

```bash
# æŸ¥çœ‹ä¿®æ”¹
git status

# æ·»åŠ æ–‡ä»¶
git add backend/master_reasoning.py
git add backend/api.py
git add backend/prompts.py
git add backend/test_reasoning.py
git add src/services/MasterService.ts

# æäº¤
git commit -m "feat: implement Chain of Thought reasoning for master services

- Add MasterReasoning class with 6-step reasoning chain
- Add Flask API for master advice
- Add prompt templates for 3 master types
- Add comprehensive test suite
- Integrate API calls into frontend MasterService"

# æ¨é€
git push origin main
```

### âœ… Day 2 éªŒæ”¶æ ‡å‡†

- [ ] å·²å®ç° MasterReasoning ç±»ï¼ˆ6æ­¥æ¨ç†é“¾ï¼‰
- [ ] å·²åˆ›å»º Flask API æ¥å£
- [ ] å·²ä¼˜åŒ– Prompt æ¨¡æ¿
- [ ] å·²åˆ›å»ºæµ‹è¯•è„šæœ¬
- [ ] æµ‹è¯•é€šè¿‡ï¼ˆ3ç§å¤§å¸ˆç±»å‹ï¼‰
- [ ] ä»£ç å·²æäº¤å¹¶æ¨é€åˆ° GitHub
- [ ] API å¯ä»¥æ­£å¸¸å“åº”è¯·æ±‚

---

## ğŸ“… Day 3-7: ç»§ç»­æ‰§è¡Œ

ç”±äºç¯‡å¹…é™åˆ¶ï¼ŒDay 3-7 çš„è¯¦ç»†æ­¥éª¤è¯·å‚è€ƒï¼š
- **Day 3**: INTELLIGENCE_UPGRADE.md ä¸­çš„"Day 3: è®°å¿†ç³»ç»Ÿæ¥å…¥"
- **Day 4**: INTELLIGENCE_UPGRADE.md ä¸­çš„"Day 4: åª’ä½“å¤„ç†é›†æˆ"
- **Day 5**: INTELLIGENCE_UPGRADE.md ä¸­çš„"Day 5: åœ°å›¾æ¨èé›†æˆ"
- **Day 6**: INTELLIGENCE_UPGRADE.md ä¸­çš„"Day 6: UI/UX ä¼˜åŒ–"
- **Day 7**: INTELLIGENCE_UPGRADE.md ä¸­çš„"Day 7: æµ‹è¯•ä¸ä¸Šçº¿"

---

## ğŸ”„ Git åŒæ­¥æµç¨‹

### æ¯æ—¥åŒæ­¥

```bash
# æ—©ä¸Šå¼€å§‹å·¥ä½œå‰
git pull origin main

# å·¥ä½œä¸­å®šæœŸæäº¤
git add .
git commit -m "feat: [æè¿°]"

# æ™šä¸Šä¸‹ç­å‰æ¨é€
git push origin main
```

### æäº¤è§„èŒƒ

```bash
# åŠŸèƒ½å¼€å‘
git commit -m "feat: add memory activation system"

# Bug ä¿®å¤
git commit -m "fix: resolve memory retrieval timeout issue"

# æ–‡æ¡£æ›´æ–°
git commit -m "docs: update execution playbook"

# é‡æ„
git commit -m "refactor: optimize master reasoning logic"

# æµ‹è¯•
git commit -m "test: add unit tests for memory service"
```

### å¤‡ä»½ç­–ç•¥

```bash
# æ¯å¤©ç»“æŸæ—¶åˆ›å»ºå¤‡ä»½
tar -czf backup_$(date +%Y%m%d).tar.gz \
  src/ backend/ *.md package.json

# ä¸Šä¼ åˆ°äº‘ç«¯ï¼ˆå¯é€‰ï¼‰
# aws s3 cp backup_$(date +%Y%m%d).tar.gz s3://yanbao-backup/
```

---

## âœ… æ€»éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½å®Œæ•´æ€§
- [ ] æ‰€æœ‰ 7 å¤©ä»»åŠ¡å·²å®Œæˆ
- [ ] å››å¤§æ™ºèƒ½åŒ–å‡çº§å·²å®ç°
- [ ] æ‰€æœ‰ä»£ç å·²æäº¤åˆ° GitHub

### æŠ€æœ¯æŒ‡æ ‡
- [ ] è®°å¿†æ£€ç´¢å»¶è¿Ÿ < 100ms
- [ ] æ¨ç†é“¾å“åº”æ—¶é—´ < 3s
- [ ] æ¨èå‡†ç¡®ç‡ > 70%
- [ ] ç³»ç»Ÿå¯ç”¨æ€§ > 99.9%

### æ™ºèƒ½åŒ–ç¨‹åº¦
- [ ] ä¸»åŠ¨æ¨èå‘½ä¸­ç‡ > 60%
- [ ] ç”¨æˆ·æ¥å—ç‡ > 50%
- [ ] è®°å¿†å¬å›å‡†ç¡®ç‡ > 80%

---

**æ–‡æ¡£ä½œè€…**: Jason Tsao  
**æ›´æ–°æ—¶é—´**: 2026å¹´1æœˆ17æ—¥  
**ç‰ˆæœ¬**: 1.0

**æŒ‰ç…§è¿™ä¸ªæ‰‹å†Œæ‰§è¡Œï¼Œè®© yanbao AI çœŸæ­£"æ´»"èµ·æ¥ï¼** ğŸš€
